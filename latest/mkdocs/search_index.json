{
    "docs": [
        {
            "location": "/", 
            "text": "MCTS\n\n\nThis package implements the Monte-Carlo Tree Search algorithm in Julia for solving Markov decision processes (MDPs). The user should define the problem according to the API in \nPOMDPs.jl\n. Examples of problem definitions can be found in \nPOMDPModels.jl\n. For an extensive tutorial, see \nthis\n notebook.\n\n\nSpecial thanks to Jon Cox for writing the original version of this code.\n\n\n\n\nInstallation\n\n\nAfter installing \nPOMDPs.jl\n, start Julia and run the following command:\n\n\nusing POMDPs\nPOMDPs.add(\nMCTS\n)\n\n\n\n\n\n\nUsage\n\n\nProblems should be defined using the \nPOMDPs.jl interface\n. Use of the \nGenerativeModels.jl\n package to define state transition and reward sampling. The following functions should be implemented for the problem:\n\n\ngenerate_sr(mdp::MDP, s, a, rng::AbstractRNG)\ndiscount(mdp::MDP)\nactions(mdp::MDP)\nactions(mdp::MDP, s::State, as::ActionSpace)\nisterminal(mdp::MDP, s::State)\n\n\n\n\nTo use the default random rollout policy, an action space sampling function\n\n\nrand(rng::AbstractRNG, action_space::AbstractSpace)\n\n\n\n\nmust also be implemented.\n\n\nProblems that do \nnot\n use \nGenerativeModels.jl\n can be used with MCTS.jl, but must have the following three functions defined \ninstead of\n \ngenerate_sr\n.\n\n\ntransition(mdp::MDP, s, a, d::AbstractDistribution)\nrand(rng::AbstractRNG, d::AbstractDistribution)\nreward(mdp::MDP, s, a)\n\n\n\n\nOnce the above functions are defined, the solver can be called with the following syntax:\n\n\nusing MyMDP # module containing your MDP type and the associated functions\nusing MCTS\n\nmdp = MyMDP() # initializes the MDP\nsolver = MCTSSolver(n_iterations=50, depth=20, exploration_constant=5.0) # initializes the Solver type\npolicy = solve(solver, mdp) # initializes the policy\n\n\n\n\nBy default, the solver will use a random policy for rollouts. If you want to pass in a custom rollout policy you can run:\n\n\nrollout_policy = MyCustomPolicy() # of type Policy, and has method action(rollout_policy::MyCustomPolicy, s::State)\nsolver = MCTSSolver(estimate_value=RolloutEstimator(rollout_policy)) # default solver parameters will be used n_iterations=100, depth=10, exploration_constant=1.0\npolicy = solve(solver, mdp)\n\n\n\n\nSince Monte-Carlo Tree Search is an online method, the solve function simply specifies the mdp model to the solver (which is embedded in the policy object). (Note that an MCTSPolicy can also be constructed directly without calling \nsolve()\n.) The computation is done during calls to the action function. To extract the policy for a given state, simply call the action function:\n\n\ns = create_state(mdp) # this can be any valid state\na = action(polciy, s) # returns the action for state s\n\n\n\n\n\n\nSolver Variants\n\n\nThere are currently two variants of the MCTS solver. They are documented in detail in the following sections:\n\n\n\n\nVanilla\n\n\nDouble Progressive Widening\n\n\n\n\n\n\nVisualization\n\n\nAn example of visualization of the search tree in a jupyter notebook is \nhere\n.\n\n\n\n\nIncorporating Additional Prior Knowledge\n\n\nAn example of incorporating additional prior domain knowledge (to initialize Q and N) and to get an estimate of the value is \nhere\n.", 
            "title": "Home"
        }, 
        {
            "location": "/#mcts", 
            "text": "This package implements the Monte-Carlo Tree Search algorithm in Julia for solving Markov decision processes (MDPs). The user should define the problem according to the API in  POMDPs.jl . Examples of problem definitions can be found in  POMDPModels.jl . For an extensive tutorial, see  this  notebook.  Special thanks to Jon Cox for writing the original version of this code.", 
            "title": "MCTS"
        }, 
        {
            "location": "/#installation", 
            "text": "After installing  POMDPs.jl , start Julia and run the following command:  using POMDPs\nPOMDPs.add( MCTS )", 
            "title": "Installation"
        }, 
        {
            "location": "/#usage", 
            "text": "Problems should be defined using the  POMDPs.jl interface . Use of the  GenerativeModels.jl  package to define state transition and reward sampling. The following functions should be implemented for the problem:  generate_sr(mdp::MDP, s, a, rng::AbstractRNG)\ndiscount(mdp::MDP)\nactions(mdp::MDP)\nactions(mdp::MDP, s::State, as::ActionSpace)\nisterminal(mdp::MDP, s::State)  To use the default random rollout policy, an action space sampling function  rand(rng::AbstractRNG, action_space::AbstractSpace)  must also be implemented.  Problems that do  not  use  GenerativeModels.jl  can be used with MCTS.jl, but must have the following three functions defined  instead of   generate_sr .  transition(mdp::MDP, s, a, d::AbstractDistribution)\nrand(rng::AbstractRNG, d::AbstractDistribution)\nreward(mdp::MDP, s, a)  Once the above functions are defined, the solver can be called with the following syntax:  using MyMDP # module containing your MDP type and the associated functions\nusing MCTS\n\nmdp = MyMDP() # initializes the MDP\nsolver = MCTSSolver(n_iterations=50, depth=20, exploration_constant=5.0) # initializes the Solver type\npolicy = solve(solver, mdp) # initializes the policy  By default, the solver will use a random policy for rollouts. If you want to pass in a custom rollout policy you can run:  rollout_policy = MyCustomPolicy() # of type Policy, and has method action(rollout_policy::MyCustomPolicy, s::State)\nsolver = MCTSSolver(estimate_value=RolloutEstimator(rollout_policy)) # default solver parameters will be used n_iterations=100, depth=10, exploration_constant=1.0\npolicy = solve(solver, mdp)  Since Monte-Carlo Tree Search is an online method, the solve function simply specifies the mdp model to the solver (which is embedded in the policy object). (Note that an MCTSPolicy can also be constructed directly without calling  solve() .) The computation is done during calls to the action function. To extract the policy for a given state, simply call the action function:  s = create_state(mdp) # this can be any valid state\na = action(polciy, s) # returns the action for state s", 
            "title": "Usage"
        }, 
        {
            "location": "/#solver-variants", 
            "text": "There are currently two variants of the MCTS solver. They are documented in detail in the following sections:   Vanilla  Double Progressive Widening", 
            "title": "Solver Variants"
        }, 
        {
            "location": "/#visualization", 
            "text": "An example of visualization of the search tree in a jupyter notebook is  here .", 
            "title": "Visualization"
        }, 
        {
            "location": "/#incorporating-additional-prior-knowledge", 
            "text": "An example of incorporating additional prior domain knowledge (to initialize Q and N) and to get an estimate of the value is  here .", 
            "title": "Incorporating Additional Prior Knowledge"
        }, 
        {
            "location": "/vanilla/", 
            "text": "Vanilla\n\n\nThe \"vanilla\" solver is the most basic version of MCTS. It works well with small discrete state and action spaces.\n\n\nThe solver fields are used to specify solver parameters. All of them can be specified as keyword arguments to the solver constructor.\n\n\n#\n\n\nMCTS.MCTSSolver\n \n \nType\n.\n\n\nMCTS solver type\n\n\nFields:\n\n\nn_iterations::Int64\n    Number of iterations during each action() call.\n    default: 100\n\ndepth::Int64:\n    Maximum rollout horizon and tree depth.\n    default: 10\n\nexploration_constant::Float64: \n    Specifies how much the solver should explore.\n    In the UCB equation, Q + c*sqrt(log(t/N)), c is the exploration constant.\n    default: 1.0\n\nrng::AbstractRNG:\n    Random number generator\n\nestimate_value::Any (rollout policy)\n    Function, object, or number used to estimate the value at the leaf nodes.\n    If this is a function `f`, `f(mdp, s, depth)` will be called to estimate the value.\n    If this is an object `o`, `estimate_value(o, mdp, s, depth)` will be called.\n    If this is a number, the value will be set to that number\n    default: RolloutEstimator(RandomSolver(rng))\n\ninit_Q::Any\n    Function, object, or number used to set the initial Q(s,a) value at a new node.\n    If this is a function `f`, `f(mdp, s, a)` will be called to set the value.\n    If this is an object `o`, `init_Q(o, mdp, s, a)` will be called.\n    If this is a number, Q will be set to that number\n    default: 0.0\n\ninit_N::Any\n    Function, object, or number used to set the initial N(s,a) value at a new node.\n    If this is a function `f`, `f(mdp, s, a)` will be called to set the value.\n    If this is an object `o`, `init_N(o, mdp, s, a)` will be called.\n    If this is a number, N will be set to that number\n    default: 0\n\nenable_tree_vis::Bool:\n    If this is true, extra information needed for tree visualization will\n    be recorded. If it is false, the tree cannot be visualized.\n    default: false\n\n\n\n\nsource", 
            "title": "Vanilla"
        }, 
        {
            "location": "/vanilla/#vanilla", 
            "text": "The \"vanilla\" solver is the most basic version of MCTS. It works well with small discrete state and action spaces.  The solver fields are used to specify solver parameters. All of them can be specified as keyword arguments to the solver constructor.  #  MCTS.MCTSSolver     Type .  MCTS solver type  Fields:  n_iterations::Int64\n    Number of iterations during each action() call.\n    default: 100\n\ndepth::Int64:\n    Maximum rollout horizon and tree depth.\n    default: 10\n\nexploration_constant::Float64: \n    Specifies how much the solver should explore.\n    In the UCB equation, Q + c*sqrt(log(t/N)), c is the exploration constant.\n    default: 1.0\n\nrng::AbstractRNG:\n    Random number generator\n\nestimate_value::Any (rollout policy)\n    Function, object, or number used to estimate the value at the leaf nodes.\n    If this is a function `f`, `f(mdp, s, depth)` will be called to estimate the value.\n    If this is an object `o`, `estimate_value(o, mdp, s, depth)` will be called.\n    If this is a number, the value will be set to that number\n    default: RolloutEstimator(RandomSolver(rng))\n\ninit_Q::Any\n    Function, object, or number used to set the initial Q(s,a) value at a new node.\n    If this is a function `f`, `f(mdp, s, a)` will be called to set the value.\n    If this is an object `o`, `init_Q(o, mdp, s, a)` will be called.\n    If this is a number, Q will be set to that number\n    default: 0.0\n\ninit_N::Any\n    Function, object, or number used to set the initial N(s,a) value at a new node.\n    If this is a function `f`, `f(mdp, s, a)` will be called to set the value.\n    If this is an object `o`, `init_N(o, mdp, s, a)` will be called.\n    If this is a number, N will be set to that number\n    default: 0\n\nenable_tree_vis::Bool:\n    If this is true, extra information needed for tree visualization will\n    be recorded. If it is false, the tree cannot be visualized.\n    default: false  source", 
            "title": "Vanilla"
        }, 
        {
            "location": "/dpw/", 
            "text": "Double Progressive Widening\n\n\nThe double progressive widening DPW solver is useful for problems with large (e.g. continuous) state and action spaces. It gradually expands the tree's branching factor so that the algorithm explores deeply even with large spaces.\n\n\nSee the papers at \nhttps://hal.archives-ouvertes.fr/file/index/docid/542673/filename/c0mcts.pdf\n and \nhttp://arxiv.org/abs/1405.5498\n for a description.\n\n\nThe solver fields are used to specify solver parameters. All of them can be specified as keyword arguments to the solver constructor.\n\n\n#\n\n\nMCTS.DPWSolver\n \n \nType\n.\n\n\nMCTS solver with DPW\n\n\nFields:\n\n\ndepth::Int64:\n    Maximum rollout horizon and tree depth.\n    default: 10\n\nexploration_constant::Float64: \n    Specified how much the solver should explore.\n    In the UCB equation, Q + c*sqrt(log(t/N)), c is the exploration constant.\n    default: 1.0\n\nn_iterations::Int64\n    Number of iterations during each action() call.\n    default: 100\n\nk_action::Float64\nalpha_action::Float64\nk_state::Float64\nalpha_state::Float64\n    These constants control the double progressive widening. A new state\n    or action will be added if the number of children is less than or equal to kN^alpha.\n    defaults: k:10, alpha:0.5\n\nrng::AbstractRNG:\n    Random number generator\n\nestimate_value::Any (rollout policy)\n    Function, object, or number used to estimate the value at the leaf nodes.\n    If this is a function `f`, `f(mdp, s, depth)` will be called to estimate the value.\n    If this is an object `o`, `estimate_value(o, mdp, s, depth)` will be called.\n    If this is a number, the value will be set to that number.\n    default: RolloutEstimator(RandomSolver(rng))\n\ninit_Q::Any\n    Function, object, or number used to set the initial Q(s,a) value at a new node.\n    If this is a function `f`, `f(mdp, s, a)` will be called to set the value.\n    If this is an object `o`, `init_Q(o, mdp, s, a)` will be called.\n    If this is a number, Q will always be set to that number.\n    default: 0.0\n\ninit_N::Any\n    Function, object, or number used to set the initial N(s,a) value at a new node.\n    If this is a function `f`, `f(mdp, s, a)` will be called to set the value.\n    If this is an object `o`, `init_N(o, mdp, s, a)` will be called.\n    If this is a number, N will always be set to that number.\n    default: 0\n\nnext_action::Any\n    Function or object used to choose the next action to be considered for progressive widening.\n    The next action is determined based on the MDP, the state, `s`, and the current `DPWStateNode`, `snode`.\n    If this is a function `f`, `f(mdp, s, snode)` will be called to set the value.\n    If this is an object `o`, `next_action(o, mdp, s, snode)` will be called.\n    default: RandomActionGenerator(rng)\n\n\n\n\nsource", 
            "title": "DPW"
        }, 
        {
            "location": "/dpw/#double-progressive-widening", 
            "text": "The double progressive widening DPW solver is useful for problems with large (e.g. continuous) state and action spaces. It gradually expands the tree's branching factor so that the algorithm explores deeply even with large spaces.  See the papers at  https://hal.archives-ouvertes.fr/file/index/docid/542673/filename/c0mcts.pdf  and  http://arxiv.org/abs/1405.5498  for a description.  The solver fields are used to specify solver parameters. All of them can be specified as keyword arguments to the solver constructor.  #  MCTS.DPWSolver     Type .  MCTS solver with DPW  Fields:  depth::Int64:\n    Maximum rollout horizon and tree depth.\n    default: 10\n\nexploration_constant::Float64: \n    Specified how much the solver should explore.\n    In the UCB equation, Q + c*sqrt(log(t/N)), c is the exploration constant.\n    default: 1.0\n\nn_iterations::Int64\n    Number of iterations during each action() call.\n    default: 100\n\nk_action::Float64\nalpha_action::Float64\nk_state::Float64\nalpha_state::Float64\n    These constants control the double progressive widening. A new state\n    or action will be added if the number of children is less than or equal to kN^alpha.\n    defaults: k:10, alpha:0.5\n\nrng::AbstractRNG:\n    Random number generator\n\nestimate_value::Any (rollout policy)\n    Function, object, or number used to estimate the value at the leaf nodes.\n    If this is a function `f`, `f(mdp, s, depth)` will be called to estimate the value.\n    If this is an object `o`, `estimate_value(o, mdp, s, depth)` will be called.\n    If this is a number, the value will be set to that number.\n    default: RolloutEstimator(RandomSolver(rng))\n\ninit_Q::Any\n    Function, object, or number used to set the initial Q(s,a) value at a new node.\n    If this is a function `f`, `f(mdp, s, a)` will be called to set the value.\n    If this is an object `o`, `init_Q(o, mdp, s, a)` will be called.\n    If this is a number, Q will always be set to that number.\n    default: 0.0\n\ninit_N::Any\n    Function, object, or number used to set the initial N(s,a) value at a new node.\n    If this is a function `f`, `f(mdp, s, a)` will be called to set the value.\n    If this is an object `o`, `init_N(o, mdp, s, a)` will be called.\n    If this is a number, N will always be set to that number.\n    default: 0\n\nnext_action::Any\n    Function or object used to choose the next action to be considered for progressive widening.\n    The next action is determined based on the MDP, the state, `s`, and the current `DPWStateNode`, `snode`.\n    If this is a function `f`, `f(mdp, s, snode)` will be called to set the value.\n    If this is an object `o`, `next_action(o, mdp, s, snode)` will be called.\n    default: RandomActionGenerator(rng)  source", 
            "title": "Double Progressive Widening"
        }
    ]
}