{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incorporating Domain Knowledge\n",
    "\n",
    "Aside from tuning the solver parameters (c, k, alpha), MCTS currently offers several means of incorporating domain knowledge. The following solver parameters control the planner's behavior:\n",
    "\n",
    "- `estimate_value` determines how the value is estimated at the leaf nodes (this is usually done using a rollout simulation).\n",
    "- `init_N` and `init_Q` determine how N(s,a) and Q(s,a) are initialized when a new node is created.\n",
    "- `next_action` determines which new actions are tried in double progressive widening\n",
    "\n",
    "There are three ways of specifying these parameters: 1) with constant values, 2) with functions, and 3) with custom objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MCTS\n",
    "importall POMDPs\n",
    "using POMDPModels\n",
    "mdp = GridWorld();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Values\n",
    "\n",
    "`init_N`, `init_Q`, and `estimate_value` can be set with constant values (though this is a bad idea for `estimate_value`. `next_action` cannot be specified in this way. The following code sets all new N to 3 and all new Q to 11.73 for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-Action Nodes\n",
      "s:POMDPModels.GridWorldState(1, 1, false), a:up Q:8.7975 N:4\n",
      "s:POMDPModels.GridWorldState(1, 1, false), a:down Q:5.027142857142857 N:7\n",
      "s:POMDPModels.GridWorldState(1, 1, false), a:left Q:11.73 N:3\n",
      "s:POMDPModels.GridWorldState(1, 1, false), a:right Q:11.73 N:3\n",
      "s:POMDPModels.GridWorldState(1, 2, false), a:up Q:11.73 N:3\n",
      "s:POMDPModels.GridWorldState(1, 2, false), a:down Q:11.73 N:3\n",
      "s:POMDPModels.GridWorldState(1, 2, false), a:left Q:11.73 N:3\n",
      "s:POMDPModels.GridWorldState(1, 2, false), a:right Q:11.73 N:3\n"
     ]
    }
   ],
   "source": [
    "solver = MCTSSolver(n_iterations=3, depth=4,\n",
    "                    init_N=3,\n",
    "                    init_Q=11.73)\n",
    "policy = solve(solver, mdp)\n",
    "action(policy, GridWorldState(1,1))\n",
    "println(\"State-Action Nodes\")\n",
    "for (s,sn) in policy.tree\n",
    "    for san in sn.sanodes\n",
    "        println(\"s:$s, a:$(san.action) Q:$(san.Q) N:$(san.N)\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "`init_N`, `init_Q`, `estimate_value`, and `next_action` can also be functions. The following code will\n",
    "\n",
    "- initialize Q to 0.0 everywhere except state [1,2] where it will be 11.73\n",
    "- initialize N to 0 everywhere except state [1,2] where it will be 3\n",
    "- estimate the value to be 10 divided by the manhattan distance to state [9,3]\n",
    "- always choose action \"up\" first in double progressive widening\n",
    "\n",
    "Note: the `?` below is part of the [ternary operator](http://docs.julialang.org/en/release-0.5/manual/control-flow/#control-flow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_Q(mdp, s, a) = s == GridWorldState(1,2) ? 11.73 : 0.0\n",
    "special_N(mdp, s, a) = s == GridWorldState(1,2) ? 3 : 0\n",
    "\n",
    "function manhattan_value(mdp, s, depth) # depth is the solver `depth` parameter less the number of timesteps that have already passed (it can be ignored in many cases)\n",
    "    m_dist = abs(s.x-9)+abs(s.y-3)\n",
    "    val = 10.0/m_dist\n",
    "    println(\"Set value for $s to $val\") # this is not necessary - just shows that it's working later\n",
    "    return val\n",
    "end\n",
    "\n",
    "function up_priority(mdp, s, snode) # snode is the state node of type DPWStateNode\n",
    "    if haskey(snode.A, GridWorldAction(:up)) # \"up\" is already there\n",
    "        return GridWorldAction(rand([:up, :left, :down, :right])) # add a random action\n",
    "    else\n",
    "        return GridWorldAction(:up)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set value for POMDPModels.GridWorldState(1, 1, false) to 1.0\n",
      "Set value for POMDPModels.GridWorldState(1, 2, false) to 1.1111111111111112\n",
      "Set value for POMDPModels.GridWorldState(2, 1, false) to 1.1111111111111112\n",
      "Set value for POMDPModels.GridWorldState(3, 1, false) to 1.25\n",
      "Set value for POMDPModels.GridWorldState(1, 3, false) to 1.25\n",
      "Set value for POMDPModels.GridWorldState(2, 2, false) to 1.25\n",
      "State-Action Nodes:\n",
      "s:POMDPModels.GridWorldState(1, 1, false), a:left Q:0.0 N:3\n",
      "s:POMDPModels.GridWorldState(1, 1, false), a:right Q:1.1039351851851853 N:3\n",
      "s:POMDPModels.GridWorldState(1, 1, false), a:up Q:0.7278935185185185 N:3\n",
      "s:POMDPModels.GridWorldState(1, 1, false), a:down Q:0.0 N:1\n",
      "s:POMDPModels.GridWorldState(3, 1, false), a:up Q:0.0 N:1\n",
      "s:POMDPModels.GridWorldState(2, 1, false), a:right Q:0.0 N:1\n",
      "s:POMDPModels.GridWorldState(2, 1, false), a:up Q:1.1875 N:2\n",
      "s:POMDPModels.GridWorldState(2, 1, false), a:down Q:0.0 N:1\n",
      "s:POMDPModels.GridWorldState(1, 2, false), a:up Q:9.094375 N:4\n"
     ]
    }
   ],
   "source": [
    "solver = DPWSolver(n_iterations=8, depth=4,\n",
    "                   init_N=special_N, init_Q=special_Q,\n",
    "                   estimate_value=manhattan_value,\n",
    "                   next_action=up_priority)\n",
    "policy = solve(solver, mdp)\n",
    "action(policy, GridWorldState(1,1))\n",
    "println(\"State-Action Nodes:\")\n",
    "for (s,sn) in policy.tree\n",
    "    for (a,san) in sn.A\n",
    "        println(\"s:$s, a:$a Q:$(san.Q) N:$(san.N)\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objects\n",
    "\n",
    "There are many cases where functions are not suitable, for example when the solver needs to be serialized. In this case, arbitrary objects may be passed to the solver to encode the behavior. The same object can be passed to multiple solver parameters to govern all of their behavior. See the docstring for the solver for more information on which functions will be called on the object(s). The following code does exactly the same thing as the function-based code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mutable struct MyHeuristic\n",
    "    target_state::GridWorldState\n",
    "    special_state::GridWorldState\n",
    "    special_Q::Float64\n",
    "    special_N::Int\n",
    "    priority_action::GridWorldAction\n",
    "    rng::AbstractRNG\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MCTS.init_Q(h::MyHeuristic, mdp::GridWorld, s, a) = s == h.special_state ? h.special_Q : 0.0\n",
    "MCTS.init_N(h::MyHeuristic, mdp::GridWorld, s, a) = s == h.special_state ? h.special_N : 0\n",
    "\n",
    "function MCTS.estimate_value(h::MyHeuristic, mdp::GridWorld, s, depth::Int)\n",
    "    targ = h.target_state\n",
    "    m_dist = abs(s.x-targ.x)+abs(s.y-targ.y)\n",
    "    val = 10.0/m_dist\n",
    "    println(\"Set value for $s to $val\") # this is not necessary - just shows that it's working later\n",
    "    return val\n",
    "end\n",
    "\n",
    "function MCTS.next_action(h::MyHeuristic, mdp::GridWorld, s, snode::DPWStateNode)\n",
    "    if haskey(snode.A, h.priority_action)\n",
    "        return GridWorldAction(rand(h.rng, [:up, :left, :down, :right])) # add a random other action\n",
    "    else\n",
    "        return h.priority_action\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set value for POMDPModels.GridWorldState(1, 1, false) to 1.0\n",
      "Set value for POMDPModels.GridWorldState(1, 2, false) to 1.1111111111111112\n",
      "Set value for POMDPModels.GridWorldState(2, 1, false) to 1.1111111111111112\n",
      "Set value for POMDPModels.GridWorldState(2, 2, false) to 1.25\n",
      "Set value for POMDPModels.GridWorldState(3, 1, false) to 1.25\n",
      "State-Action Nodes:\n",
      "s:POMDPModels.GridWorldState(1, 1, false), a:left Q:0.0 N:4\n",
      "s:POMDPModels.GridWorldState(1, 1, false), a:right Q:0.5459201388888888 N:4\n",
      "s:POMDPModels.GridWorldState(1, 1, false), a:up Q:1.063637152777778 N:2\n",
      "s:POMDPModels.GridWorldState(1, 1, false), a:down Q:0.0 N:4\n",
      "s:POMDPModels.GridWorldState(2, 1, false), a:left Q:0.0 N:1\n",
      "s:POMDPModels.GridWorldState(2, 1, false), a:right Q:1.1875 N:1\n",
      "s:POMDPModels.GridWorldState(2, 1, false), a:up Q:1.1875 N:1\n",
      "s:POMDPModels.GridWorldState(2, 1, false), a:down Q:1.128125 N:1\n",
      "s:POMDPModels.GridWorldState(1, 2, false), a:up Q:8.7975 N:4\n"
     ]
    }
   ],
   "source": [
    "heur = MyHeuristic(GridWorldState(9,3), GridWorldState(1,2), 11.73, 3, GridWorldAction(:up), Base.GLOBAL_RNG)\n",
    "solver = DPWSolver(n_iterations=8, depth=4,\n",
    "                   init_N=heur, init_Q=heur,\n",
    "                   estimate_value=heur,\n",
    "                   next_action=heur)\n",
    "policy = solve(solver, mdp)\n",
    "action(policy, GridWorldState(1,1))\n",
    "println(\"State-Action Nodes:\")\n",
    "for (s,sn) in policy.tree\n",
    "    for (a,san) in sn.A\n",
    "        println(\"s:$s, a:$a Q:$(san.Q) N:$(san.N)\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollouts\n",
    "\n",
    "The most common way to estimate the value of a state node is with rollout simulations. This can be done with an arbitrary policy or solver by passing a `RolloutEstimator` object as the `estimate_value` parameter. The following code does this with a policy that moves towards state [9,3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mutable struct SeekTarget <: Policy\n",
    "    target::GridWorldState\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function POMDPs.action(p::SeekTarget, s::GridWorldState, a::GridWorldAction=GridWorldAction(:up))\n",
    "    if p.target.x > s.x\n",
    "        return GridWorldAction(:right)\n",
    "    elseif p.target.x < s.x\n",
    "        return GridWorldAction(:left)\n",
    "    elseif p.target.y > s.y\n",
    "        return GridWorldAction(:up)\n",
    "    else\n",
    "        return GridWorldAction(:down)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-Action Nodes\n",
      "s:POMDPModels.GridWorldState(4,2,false), a:POMDPModels.GridWorldAction(:up) Q:0.0 N:0\n",
      "s:POMDPModels.GridWorldState(4,2,false), a:POMDPModels.GridWorldAction(:down) Q:0.0 N:0\n",
      "s:POMDPModels.GridWorldState(4,2,false), a:POMDPModels.GridWorldAction(:left) Q:0.0 N:0\n",
      "s:POMDPModels.GridWorldState(4,2,false), a:POMDPModels.GridWorldAction(:right) Q:0.0 N:0\n",
      "s:POMDPModels.GridWorldState(4,1,false), a:POMDPModels.GridWorldAction(:up) Q:5.688000922764596 N:1\n",
      "s:POMDPModels.GridWorldState(4,1,false), a:POMDPModels.GridWorldAction(:down) Q:0.0 N:0\n",
      "s:POMDPModels.GridWorldState(4,1,false), a:POMDPModels.GridWorldAction(:left) Q:0.0 N:0\n",
      "s:POMDPModels.GridWorldState(4,1,false), a:POMDPModels.GridWorldAction(:right) Q:0.0 N:0\n",
      "s:POMDPModels.GridWorldState(5,1,false), a:POMDPModels.GridWorldAction(:up) Q:7.350918906249999 N:1\n",
      "s:POMDPModels.GridWorldState(5,1,false), a:POMDPModels.GridWorldAction(:down) Q:0.0 N:20\n",
      "s:POMDPModels.GridWorldState(5,1,false), a:POMDPModels.GridWorldAction(:left) Q:-4.172483313482347 N:1\n",
      "s:POMDPModels.GridWorldState(5,1,false), a:POMDPModels.GridWorldAction(:right) Q:5.403600876626366 N:1\n",
      "s:POMDPModels.GridWorldState(5,2,false), a:POMDPModels.GridWorldAction(:up) Q:0.0 N:0\n",
      "s:POMDPModels.GridWorldState(5,2,false), a:POMDPModels.GridWorldAction(:down) Q:0.0 N:0\n",
      "s:POMDPModels.GridWorldState(5,2,false), a:POMDPModels.GridWorldAction(:left) Q:0.0 N:0\n",
      "s:POMDPModels.GridWorldState(5,2,false), a:POMDPModels.GridWorldAction(:right) Q:0.0 N:0\n"
     ]
    }
   ],
   "source": [
    "solver = MCTSSolver(n_iterations=5, depth=20,\n",
    "                    estimate_value=RolloutEstimator(SeekTarget(GridWorldState(9,3))))\n",
    "policy = solve(solver, mdp)\n",
    "action(policy, GridWorldState(5,1))\n",
    "println(\"State-Action Nodes\")\n",
    "for (s,sn) in policy.tree\n",
    "    for san in sn.sanodes\n",
    "        println(\"s:$s, a:$(san.action) Q:$(san.Q) N:$(san.N)\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
